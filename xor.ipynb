{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals\n",
    "\n",
    "*Main piece*: **ComputationGraph** (created when dynet is imported, is in the background as a singleton object)\n",
    "\n",
    "ComputationGraph = **expressions** (related to the inputs and outputs of the network) + **ParameterCollection** (containing the parameters that are optimized over time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The XOR problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: http://dynet.readthedocs.io/en/latest/tutorials_notebooks/tutorial-1-xor.html\n",
    "\n",
    "Consider a model for solving the “xor” problem. The network has two inputs, which can be 0 or 1, and a single output which should be the xor of the two inputs. We will model this as a multi-layer perceptron with a single hidden layer.\n",
    "\n",
    "Let $x = x_1, x_2$ be our input. We will have a hidden layer of 8 nodes, and an output layer of a single node. The activation on the hidden layer will be a tanh. Our network will then be:\n",
    "\n",
    "$\\sigma(V(\\tanh(Wx+b)))$\n",
    "\n",
    "Where $W$ is a $8 \\times 2$ matrix, $V$ is an $8 \\times 1$ matrix, and $b$ is an 8-dim vector.\n",
    "\n",
    "We want the output to be either 0 or 1, so we take the output layer to be the logistic-sigmoid function, $\\sigma(x)$, that takes values between $-\\infty$ and $+\\infty$ and returns numbers in $[0,1]$.\n",
    "\n",
    "We will begin by defining the model and the computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dynet as dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a parameter collection and add the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = dy.ParameterCollection()\n",
    "W = m.add_parameters((8,2))  # 8x2 matrix\n",
    "V = m.add_parameters((1,8))  # 8x1 matrix\n",
    "b = m.add_parameters((8))    # 8-dim vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new computation graph. Not strictly needed here, but good practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_dynet.ComputationGraph at 0x7f96ac7bd090>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dy.renew_cg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model parameters can be used as expressions in the computation graph. We now make use of V, W, and b in order to create the complete expression for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dy.vecInput(2)  # an input vector of size 2. Also an expression.\n",
    "output = dy.logistic(V*(dy.tanh((W*x)+b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now query our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.659337043762207"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.set([0,0])\n",
    "output.value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to be able to define a loss, so we need an input expression to work against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dy.scalarInput(0)  # this will hold the correct answer\n",
    "loss = dy.binary_log_loss(output, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3439751863479614\n",
      "0.30219602584838867\n"
     ]
    }
   ],
   "source": [
    "x.set([1,0])\n",
    "y.set(0)\n",
    "print(loss.value())  # xor(1, 0) = 1, so y = 0 --> hight loss\n",
    "\n",
    "y.set(1)\n",
    "print(loss.value())  # xor(1, 0) = 1, so y = 1 --> lower loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "We now want to set the parameter weights such that the loss is minimized. \n",
    "\n",
    "For this, we will use a trainer object. A trainer is constructed with respect to the parameters of a given model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = dy.SimpleSGDTrainer(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the trainer, we need to:\n",
    "* **call the `forward_scalar`** method of `ComputationGraph`. This will run a forward pass through the network, calculating all the intermediate values until the last one (`loss`, in our case), and then convert the value to a scalar. The final output of our network **must** be a single scalar value. However, if we do not care about the value, we can just use `cg.forward()` instead of `cg.forward_sclar()`.\n",
    "* **call the `backward`** method of `ComputationGraph`. This will run a backward pass from the last node, calculating the gradients with respect to minimizing the last expression (in our case we want to minimize the loss). The gradients are stored in the parameter collection, and we can now let the `trainer` take care of the optimization step.\n",
    "* **call `trainer.update()`** to optimize the values with respect to the latest gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the loss before step is: 0.30219602584838867\n",
      "the loss after step is: 0.262879341840744\n"
     ]
    }
   ],
   "source": [
    "x.set([1,0])\n",
    "y.set(1)\n",
    "loss_value = loss.value() # this performs a forward through the network.\n",
    "print(\"the loss before step is:\",loss_value)\n",
    "\n",
    "# now do an optimization step\n",
    "loss.backward()  # compute the gradients\n",
    "trainer.update()\n",
    "\n",
    "# see how it affected the loss:\n",
    "loss_value = loss.value(recalculate=True) # recalculate=True means \"don't use precomputed value\"\n",
    "print(\"the loss after step is:\",loss_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization step indeed made the loss decrease. We now need to run this in a loop.\n",
    "To this end, we will create a `training set`, and iterate over it.\n",
    "\n",
    "For the xor problem, the training instances are easy to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_xor_instances(num_rounds=2000):\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for round in range(num_rounds):\n",
    "        for x1 in 0,1:\n",
    "            for x2 in 0,1:\n",
    "                answer = 0 if x1==x2 else 1\n",
    "                questions.append((x1,x2))\n",
    "                answers.append(answer)\n",
    "    return questions, answers \n",
    "\n",
    "questions, answers = create_xor_instances()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now feed each question / answer pair to the network, and try to minimize the loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss is: 0.7172141650319099\n",
      "average loss is: 0.6497034585475921\n",
      "average loss is: 0.5619007851183414\n",
      "average loss is: 0.47428334223106505\n",
      "average loss is: 0.40338586842268703\n",
      "average loss is: 0.3489488636702299\n",
      "average loss is: 0.30685874039041144\n",
      "average loss is: 0.2736465136078186\n",
      "average loss is: 0.24687683546087807\n",
      "average loss is: 0.22488178132474423\n",
      "average loss is: 0.20650553174401548\n",
      "average loss is: 0.19093013191440453\n",
      "average loss is: 0.17756360904241983\n",
      "average loss is: 0.16596830448035949\n",
      "average loss is: 0.15581427463392417\n",
      "average loss is: 0.14684836332366102\n",
      "average loss is: 0.1388732382690753\n",
      "average loss is: 0.13173288560368948\n",
      "average loss is: 0.12530237336446973\n",
      "average loss is: 0.11948049996187911\n",
      "average loss is: 0.11418442966460828\n",
      "average loss is: 0.10934571184408427\n",
      "average loss is: 0.10490729135906567\n",
      "average loss is: 0.10082123500023348\n",
      "average loss is: 0.09704697908721864\n",
      "average loss is: 0.09354996512473847\n",
      "average loss is: 0.09030056918617684\n",
      "average loss is: 0.08727324966291365\n",
      "average loss is: 0.08444586854965971\n",
      "average loss is: 0.08179914202921403\n",
      "average loss is: 0.07931619423081077\n",
      "average loss is: 0.07698219287609391\n",
      "average loss is: 0.07478404855630784\n",
      "average loss is: 0.07271016558563775\n",
      "average loss is: 0.0707502351800379\n",
      "average loss is: 0.0688950607374621\n",
      "average loss is: 0.06713641212479136\n",
      "average loss is: 0.0654669017636977\n",
      "average loss is: 0.06387987931697772\n",
      "average loss is: 0.06236934154730989\n",
      "average loss is: 0.06092985588861289\n",
      "average loss is: 0.0595564937754257\n",
      "average loss is: 0.05824477341245362\n",
      "average loss is: 0.05699061035549014\n",
      "average loss is: 0.05579027425611598\n",
      "average loss is: 0.05464035117611512\n",
      "average loss is: 0.05353771079975636\n",
      "average loss is: 0.0524794777516945\n",
      "average loss is: 0.051463005535508866\n",
      "average loss is: 0.05048585455426946\n",
      "average loss is: 0.049545772469776005\n",
      "average loss is: 0.04864067591155897\n",
      "average loss is: 0.047768635377243934\n",
      "average loss is: 0.046927861126476074\n",
      "average loss is: 0.04611669084800153\n",
      "average loss is: 0.045333578159812274\n",
      "average loss is: 0.044577083100009254\n",
      "average loss is: 0.04384586279071739\n",
      "average loss is: 0.04313866350689303\n",
      "average loss is: 0.042454313234814134\n",
      "average loss is: 0.04179171538157923\n",
      "average loss is: 0.041149842586657485\n",
      "average loss is: 0.04052773139547623\n",
      "average loss is: 0.03992447725259808\n",
      "average loss is: 0.03933923038070162\n",
      "average loss is: 0.03877119106014795\n",
      "average loss is: 0.038219606867164316\n",
      "average loss is: 0.03768376837927944\n",
      "average loss is: 0.037163006766830854\n",
      "average loss is: 0.03665669048182567\n",
      "average loss is: 0.03616422307290609\n",
      "average loss is: 0.03568504042026082\n",
      "average loss is: 0.035218608714145776\n",
      "average loss is: 0.034764422523487055\n",
      "average loss is: 0.03432200295416017\n",
      "average loss is: 0.03389089577665896\n",
      "average loss is: 0.033470669827959196\n",
      "average loss is: 0.03306091575019169\n",
      "average loss is: 0.032661244524871236\n",
      "average loss is: 0.032271286166309435\n"
     ]
    }
   ],
   "source": [
    "total_loss = 0\n",
    "seen_instances = 0\n",
    "for question, answer in zip(questions, answers):\n",
    "    x.set(question)\n",
    "    y.set(answer)\n",
    "    seen_instances += 1\n",
    "    total_loss += loss.value()\n",
    "    loss.backward()\n",
    "    trainer.update()\n",
    "    if (seen_instances > 1 and seen_instances % 100 == 0):\n",
    "        print(\"average loss is:\",total_loss / seen_instances)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our network is now trained. Let's verify that it indeed learned the xor function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1 0.9983042478561401\n",
      "1,0 0.9983420372009277\n",
      "0,0 0.0007349436054937541\n",
      "1,1 0.0017125688027590513\n"
     ]
    }
   ],
   "source": [
    "x.set([0,1])\n",
    "print(\"0,1\",output.value())\n",
    "\n",
    "x.set([1,0])\n",
    "print(\"1,0\",output.value())\n",
    "\n",
    "x.set([0,0])\n",
    "print(\"0,0\",output.value())\n",
    "\n",
    "x.set([1,1])\n",
    "print(\"1,1\",output.value())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case we are curious about the parameter values, we can query them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.62200642, -1.80553484],\n",
       "       [ 2.69344974, -1.90905726],\n",
       "       [ 0.6123637 ,  0.4486255 ],\n",
       "       [-1.75319183,  2.35105109],\n",
       "       [-2.57931232, -2.61533093],\n",
       "       [ 2.35912442, -3.0319798 ],\n",
       "       [-1.13818753, -1.09748042],\n",
       "       [ 1.15303671,  1.14093041]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.00425816, -3.13614678, -1.25156963, -2.33445621, -4.22193146,\n",
       "         3.8618443 , -1.67410648, -2.27995872]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V.value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7008389830589294,\n",
       " 0.764814019203186,\n",
       " -1.0103081464767456,\n",
       " 0.7629812359809875,\n",
       " 0.76901775598526,\n",
       " -1.0365179777145386,\n",
       " -0.45914003252983093,\n",
       " -1.862380862236023]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To summarize\n",
    "Here is a complete program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss is: 0.7239488640427589\n",
      "average loss is: 0.6833679360151291\n",
      "average loss is: 0.6269646707177162\n",
      "average loss is: 0.554561566747725\n",
      "average loss is: 0.48260273870825765\n",
      "average loss is: 0.42192818397035203\n",
      "average loss is: 0.3730690868145653\n",
      "average loss is: 0.33372908423654735\n",
      "average loss is: 0.3016572737486826\n",
      "average loss is: 0.27511965315882114\n",
      "average loss is: 0.25284487135877665\n",
      "average loss is: 0.2339041292628584\n",
      "average loss is: 0.2176117386124455\n",
      "average loss is: 0.20345395165961236\n",
      "average loss is: 0.19103979984298347\n",
      "average loss is: 0.18006722247693688\n",
      "average loss is: 0.17029953107046072\n",
      "average loss is: 0.16154883304766068\n",
      "average loss is: 0.1536641826679146\n",
      "average loss is: 0.1465229888765607\n",
      "average loss is: 0.14002469280512914\n",
      "average loss is: 0.13408605291571637\n",
      "average loss is: 0.12863758449503424\n",
      "average loss is: 0.12362083888729103\n",
      "average loss is: 0.11898630112111569\n",
      "average loss is: 0.11469174928918409\n",
      "average loss is: 0.11070096134863518\n",
      "average loss is: 0.1069826871672246\n",
      "average loss is: 0.10350982348916346\n",
      "average loss is: 0.10025874977954663\n",
      "average loss is: 0.0972087861247541\n",
      "average loss is: 0.09434174961352255\n",
      "average loss is: 0.09164158800200588\n",
      "average loss is: 0.08909407632576083\n",
      "average loss is: 0.08668656506103331\n",
      "average loss is: 0.08440776673643591\n",
      "average loss is: 0.08224757829435622\n",
      "average loss is: 0.08019692848908323\n",
      "average loss is: 0.07824765027700685\n",
      "average loss is: 0.07639237051663804\n",
      "average loss is: 0.07462441558554973\n",
      "average loss is: 0.07293773035097513\n",
      "average loss is: 0.07132680825542571\n",
      "average loss is: 0.06978663009480277\n",
      "average loss is: 0.06831261174021186\n",
      "average loss is: 0.06690055736259598\n",
      "average loss is: 0.0655466196122282\n",
      "average loss is: 0.06424726419313326\n",
      "average loss is: 0.06299923809725146\n",
      "average loss is: 0.06179954235854093\n",
      "average loss is: 0.060645407890129434\n",
      "average loss is: 0.059534273577715235\n",
      "average loss is: 0.05846376746056057\n",
      "average loss is: 0.05743168917439309\n",
      "average loss is: 0.05643599484583617\n",
      "average loss is: 0.055474783392772745\n",
      "average loss is: 0.05454628415844861\n",
      "average loss is: 0.05364884618936701\n",
      "average loss is: 0.052780927981494655\n",
      "average loss is: 0.0519410886625022\n",
      "average loss is: 0.05112797981366653\n",
      "average loss is: 0.05034033829573497\n",
      "average loss is: 0.049576979487753726\n",
      "average loss is: 0.048836791329886185\n",
      "average loss is: 0.04811872893802893\n",
      "average loss is: 0.0474218092887366\n",
      "average loss is: 0.046745107045888884\n",
      "average loss is: 0.04608775011763689\n",
      "average loss is: 0.04544891620178849\n",
      "average loss is: 0.044827828861373875\n",
      "average loss is: 0.04422375481088751\n",
      "average loss is: 0.04363600045100611\n",
      "average loss is: 0.04306390974913048\n",
      "average loss is: 0.04250686113758805\n",
      "average loss is: 0.04196426596935683\n",
      "average loss is: 0.04143556568833638\n",
      "average loss is: 0.04092023054586857\n",
      "average loss is: 0.04041775715411253\n",
      "average loss is: 0.03992766730504293\n",
      "average loss is: 0.03944950625027559\n"
     ]
    }
   ],
   "source": [
    "# define the parameters\n",
    "m = dy.ParameterCollection()\n",
    "W = m.add_parameters((8,2))\n",
    "V = m.add_parameters((1,8))\n",
    "b = m.add_parameters((8))\n",
    "\n",
    "# renew the computation graph\n",
    "dy.renew_cg()\n",
    "\n",
    "# create the network\n",
    "x = dy.vecInput(2) # an input vector of size 2.\n",
    "output = dy.logistic(V*(dy.tanh((W*x)+b)))\n",
    "# define the loss with respect to an output y.\n",
    "y = dy.scalarInput(0) # this will hold the correct answer\n",
    "loss = dy.binary_log_loss(output, y)\n",
    "\n",
    "# create training instances\n",
    "def create_xor_instances(num_rounds=2000):\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for round in range(num_rounds):\n",
    "        for x1 in 0,1:\n",
    "            for x2 in 0,1:\n",
    "                answer = 0 if x1==x2 else 1\n",
    "                questions.append((x1,x2))\n",
    "                answers.append(answer)\n",
    "    return questions, answers \n",
    "\n",
    "questions, answers = create_xor_instances()\n",
    "\n",
    "# train the network\n",
    "trainer = dy.SimpleSGDTrainer(m)\n",
    "\n",
    "total_loss = 0\n",
    "seen_instances = 0\n",
    "for question, answer in zip(questions, answers):\n",
    "    x.set(question)\n",
    "    y.set(answer)\n",
    "    seen_instances += 1\n",
    "    total_loss += loss.value()\n",
    "    loss.backward()\n",
    "    trainer.update()\n",
    "    if (seen_instances > 1 and seen_instances % 100 == 0):\n",
    "        print(\"average loss is:\",total_loss / seen_instances)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Networks\n",
    "\n",
    "Dynamic networks are very similar to static ones, but instead of creating the network once and then calling \"set\" in each training example to change the inputs, we just create a new network for each training example.\n",
    "\n",
    "We present an example below. While the value of this may not be clear in the `xor` example, the dynamic approach\n",
    "is very convenient for networks for which the structure is not fixed, such as recurrent or recursive networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss is: 0.736072083413601\n",
      "average loss is: 0.7029607383906842\n",
      "average loss is: 0.657716857890288\n",
      "average loss is: 0.5936274072155356\n",
      "average loss is: 0.522500530987978\n",
      "average loss is: 0.45876798949514824\n",
      "average loss is: 0.4062296753270285\n",
      "average loss is: 0.36354592158226295\n",
      "average loss is: 0.32861912462446424\n",
      "average loss is: 0.29967247347719966\n",
      "average loss is: 0.2753582175998864\n",
      "average loss is: 0.25467700297789025\n",
      "average loss is: 0.23688568113873212\n",
      "average loss is: 0.22142534360688712\n",
      "average loss is: 0.20786977378775676\n",
      "average loss is: 0.19588929961028043\n",
      "average loss is: 0.1852254262692569\n",
      "average loss is: 0.17567287252698507\n",
      "average loss is: 0.16706668734746544\n",
      "average loss is: 0.1592728893388994\n",
      "average loss is: 0.15218156833756005\n",
      "average loss is: 0.1457017392155037\n",
      "average loss is: 0.13975745311310594\n",
      "average loss is: 0.13428482367482503\n",
      "average loss is: 0.12922973018987105\n",
      "average loss is: 0.12454602355531488\n",
      "average loss is: 0.12019411381713493\n",
      "average loss is: 0.11613984614003649\n",
      "average loss is: 0.11235359921714226\n",
      "average loss is: 0.10880955889774487\n",
      "average loss is: 0.10548512513704238\n",
      "average loss is: 0.10236042740594713\n",
      "average loss is: 0.09941792466536616\n",
      "average loss is: 0.09664207349791575\n",
      "average loss is: 0.0940190524204102\n",
      "average loss is: 0.09153652851256387\n",
      "average loss is: 0.08918346346953783\n",
      "average loss is: 0.08694994700980667\n",
      "average loss is: 0.08482705744096818\n",
      "average loss is: 0.08280674012409872\n",
      "average loss is: 0.08088170531716925\n",
      "average loss is: 0.07904533903763097\n",
      "average loss is: 0.07729162718537588\n",
      "average loss is: 0.07561508799286623\n",
      "average loss is: 0.07401071510368233\n",
      "average loss is: 0.07247392650205217\n",
      "average loss is: 0.07100052128791155\n",
      "average loss is: 0.06958663991126135\n",
      "average loss is: 0.06822873068702578\n",
      "average loss is: 0.06692351961317472\n",
      "average loss is: 0.06566798372570809\n",
      "average loss is: 0.06445932706959026\n",
      "average loss is: 0.06329496053937067\n",
      "average loss is: 0.062172482311898085\n",
      "average loss is: 0.06108966157845729\n",
      "average loss is: 0.06004442351841135\n",
      "average loss is: 0.05903483594735071\n",
      "average loss is: 0.058059097201089725\n",
      "average loss is: 0.057115525090573674\n",
      "average loss is: 0.05620254743269955\n",
      "average loss is: 0.055318693017957304\n",
      "average loss is: 0.054462583787915776\n",
      "average loss is: 0.053632927452979075\n",
      "average loss is: 0.05282851101062988\n",
      "average loss is: 0.05204819443178936\n",
      "average loss is: 0.05129090568652221\n",
      "average loss is: 0.05055563550435507\n",
      "average loss is: 0.049841432759712916\n",
      "average loss is: 0.04914740065334574\n",
      "average loss is: 0.04847269244785587\n",
      "average loss is: 0.04781650850096796\n",
      "average loss is: 0.04717809264867456\n",
      "average loss is: 0.04655672939192051\n",
      "average loss is: 0.045951741382533155\n",
      "average loss is: 0.0453624866103443\n",
      "average loss is: 0.044788356414271556\n",
      "average loss is: 0.04422877306290963\n",
      "average loss is: 0.0436831878772892\n",
      "average loss is: 0.04315107969739592\n",
      "average loss is: 0.042631952845353226\n"
     ]
    }
   ],
   "source": [
    "import dynet as dy\n",
    "# create training instances, as before\n",
    "def create_xor_instances(num_rounds=2000):\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for round in range(num_rounds):\n",
    "        for x1 in 0,1:\n",
    "            for x2 in 0,1:\n",
    "                answer = 0 if x1==x2 else 1\n",
    "                questions.append((x1,x2))\n",
    "                answers.append(answer)\n",
    "    return questions, answers \n",
    "\n",
    "questions, answers = create_xor_instances()\n",
    "\n",
    "# create a network for the xor problem given input and output\n",
    "def create_xor_network(W, V, b, inputs, expected_answer):\n",
    "    dy.renew_cg() # new computation graph\n",
    "    x = dy.vecInput(len(inputs))\n",
    "    x.set(inputs)\n",
    "    y = dy.scalarInput(expected_answer)\n",
    "    output = dy.logistic(V*(dy.tanh((W*x)+b)))\n",
    "    loss =  dy.binary_log_loss(output, y)\n",
    "    return loss\n",
    "\n",
    "m2 = dy.ParameterCollection()\n",
    "W = m2.add_parameters((8,2))\n",
    "V = m2.add_parameters((1,8))\n",
    "b = m2.add_parameters((8))\n",
    "trainer = dy.SimpleSGDTrainer(m2)\n",
    "\n",
    "seen_instances = 0\n",
    "total_loss = 0\n",
    "for question, answer in zip(questions, answers):\n",
    "    loss = create_xor_network(W, V, b, question, answer)\n",
    "    seen_instances += 1\n",
    "    total_loss += loss.value()\n",
    "    loss.backward()\n",
    "    trainer.update()\n",
    "    if (seen_instances > 1 and seen_instances % 100 == 0):\n",
    "        print(\"average loss is:\",total_loss / seen_instances)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
